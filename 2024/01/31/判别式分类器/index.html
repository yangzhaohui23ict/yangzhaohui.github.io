<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>判别式分类器 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="判别式分类器线性判别分析​	简称LDA，是一种经典的线性学习方法，也叫fisher判别分析。 ​	LDA的思想很朴素，给定训练样例集合，把这些样例投影到一条直线上，使得同类样例的投影点尽可能接近，异类样例的投影点尽可能远离。我们希望的表现效果如下图所示：  ​	给定数据集，我们需要计算每一类数据的均值向量和协方差矩阵，然后将数据投影到直线w上。则： ​	 ​	并且，这里的4个数据都是实数。基于我们">
<meta property="og:type" content="article">
<meta property="og:title" content="判别式分类器">
<meta property="og:url" content="http://example.com/2024/01/31/%E5%88%A4%E5%88%AB%E5%BC%8F%E5%88%86%E7%B1%BB%E5%99%A8/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="判别式分类器线性判别分析​	简称LDA，是一种经典的线性学习方法，也叫fisher判别分析。 ​	LDA的思想很朴素，给定训练样例集合，把这些样例投影到一条直线上，使得同类样例的投影点尽可能接近，异类样例的投影点尽可能远离。我们希望的表现效果如下图所示：  ​	给定数据集，我们需要计算每一类数据的均值向量和协方差矩阵，然后将数据投影到直线w上。则： ​	 ​	并且，这里的4个数据都是实数。基于我们">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/pic/image-20231215105540852.png">
<meta property="og:image" content="http://example.com/pic/image-20231215105829578.png">
<meta property="og:image" content="http://example.com/pic/image-20231215110105211.png">
<meta property="og:image" content="http://example.com/pic/image-20231215110257937.png">
<meta property="og:image" content="http://example.com/pic/image-20231215111935354.png">
<meta property="og:image" content="http://example.com/pic/image-20231215112542616.png">
<meta property="og:image" content="http://example.com/pic/image-20231215113932906.png">
<meta property="og:image" content="http://example.com/pic/image-20231215114001371.png">
<meta property="og:image" content="http://example.com/pic/image-20231215114731362.png">
<meta property="og:image" content="http://example.com/pic/image-20231215130626604.png">
<meta property="og:image" content="http://example.com/pic/v2-eab35f0f8896ebe2dbf64d3c0b2bb1da_r.jpg">
<meta property="og:image" content="http://example.com/pic/v2-1eded2adc70ac4eab0afe6ec52d31892_1440w.png">
<meta property="og:image" content="http://example.com/pic/v2-8f78e1cf021f78a16bf559243fa16a87_1440w.webp">
<meta property="og:image" content="http://example.com/pic/v2-e36d0129dcd8f95d7f053c85b81387bb_1440w.webp">
<meta property="og:image" content="http://example.com/pic/v2-cacc77aeccd8e54811601e467ce8c786_1440w.png">
<meta property="og:image" content="http://example.com/pic/v2-6f6d014782dd412075f653658bd63bf0_1440w.webp">
<meta property="og:image" content="http://example.com/pic/v2-7517b4e0cdd9a1adcf4cdb42bf27162c_1440w.webp">
<meta property="og:image" content="http://example.com/pic/image-20231215150743829.png">
<meta property="og:image" content="http://example.com/pic/image-20231215150959421.png">
<meta property="og:image" content="http://example.com/pic/image-20231215151008486.png">
<meta property="og:image" content="http://example.com/pic/image-20231215151402988.png">
<meta property="og:image" content="http://example.com/pic/image-20231215151436645.png">
<meta property="og:image" content="http://example.com/pic/image-20231216100919068.png">
<meta property="og:image" content="http://example.com/pic/image-20231216101457609.png">
<meta property="og:image" content="http://example.com/pic/image-20231216101550245.png">
<meta property="og:image" content="http://example.com/pic/image-20231216102356788.png">
<meta property="og:image" content="http://example.com/pic/image-20231216103516462.png">
<meta property="og:image" content="http://example.com/pic/image-20231216103609345.png">
<meta property="og:image" content="http://example.com/pic/image-20231216103736653.png">
<meta property="og:image" content="http://example.com/pic/image-20231216105753349.png">
<meta property="og:image" content="http://example.com/pic/image-20231216105759997.png">
<meta property="og:image" content="http://example.com/pic/image-20231216105846414.png">
<meta property="og:image" content="http://example.com/pic/image-20231216105856852.png">
<meta property="og:image" content="http://example.com/pic/image-20231216105907878.png">
<meta property="og:image" content="http://example.com/pic/image-20231216113102005.png">
<meta property="og:image" content="http://example.com/pic/image-20231216114051672.png">
<meta property="og:image" content="http://example.com/pic/image-20231216114533549.png">
<meta property="og:image" content="http://example.com/pic/image-20231216115438108.png">
<meta property="og:image" content="http://example.com/pic/image-20231216115447178.png">
<meta property="og:image" content="http://example.com/pic/image-20231216115458838.png">
<meta property="og:image" content="http://example.com/pic/image-20231216115514414.png">
<meta property="og:image" content="http://example.com/pic/image-20231216115706092.png">
<meta property="og:image" content="http://example.com/pic/image-20231216131508639.png">
<meta property="og:image" content="http://example.com/pic/image-20231216131811411.png">
<meta property="og:image" content="http://example.com/pic/image-20231216132743616.png">
<meta property="og:image" content="http://example.com/pic/image-20231216134350787.png">
<meta property="og:image" content="http://example.com/pic/image-20231216134753414.png">
<meta property="og:image" content="http://example.com/pic/image-20231216134843406.png">
<meta property="og:image" content="http://example.com/pic/image-20231216144238905.png">
<meta property="og:image" content="http://example.com/pic/image-20231216144204947.png">
<meta property="og:image" content="http://example.com/pic/image-20231216144320375.png">
<meta property="article:published_time" content="2024-01-31T07:12:29.000Z">
<meta property="article:modified_time" content="2024-01-31T07:13:17.836Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="国科大模式识别与机器学习23秋季学习笔记">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/pic/image-20231215105540852.png">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-判别式分类器" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/01/31/%E5%88%A4%E5%88%AB%E5%BC%8F%E5%88%86%E7%B1%BB%E5%99%A8/" class="article-date">
  <time class="dt-published" datetime="2024-01-31T07:12:29.000Z" itemprop="datePublished">2024-01-31</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/%E8%AF%BE%E7%A8%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">课程学习笔记</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      判别式分类器
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="判别式分类器"><a href="#判别式分类器" class="headerlink" title="判别式分类器"></a>判别式分类器</h1><h2 id="线性判别分析"><a href="#线性判别分析" class="headerlink" title="线性判别分析"></a>线性判别分析</h2><p>​	简称LDA，是一种经典的线性学习方法，也叫fisher判别分析。</p>
<p>​	LDA的思想很朴素，给定训练样例集合，把这些样例投影到一条直线上，使得同类样例的投影点尽可能接近，异类样例的投影点尽可能远离。我们希望的表现效果如下图所示：</p>
<p><img src="/./../pic/image-20231215105540852.png" alt="image-20231215105540852"></p>
<p>​	给定数据集，我们需要计算每一类数据的均值向量和协方差矩阵，然后将数据投影到直线w上。则：</p>
<p>​	<img src="/./../pic/image-20231215105829578.png" alt="image-20231215105829578"></p>
<p>​	并且，这里的4个数据都是实数。基于我们前面提到的希望同类样例的投影点尽可能接近，异类样例的投影点尽可能远离的目标，给出目标函数：</p>
<p>​	其中分子部分对应类中心距离，分母部分对应同类别投影点临近程度。</p>
<p><img src="/./../pic/image-20231215110105211.png" alt="image-20231215110105211"></p>
<p>​	补充定义，重写上式：</p>
<p><img src="/./../pic/image-20231215110257937.png" alt="image-20231215110257937"></p>
<p>​	上式为LDA的目标函数，也称之为S<sub>b</sub>和S<sub>w</sub>的广义瑞利商。显然这里定义中的S<sub>b</sub>和S<sub>w</sub>都是基于给定数据集已知的。此时我们的目标就是计算W.</p>
<p>​	显然，上式的分子分母都是关于w的二次项，因此这个式子的解只会和W的方向有关，与w的长度无关，因此，我们可以再次重构上式：</p>
<p>​	因为S<sub>b</sub>和S<sub>w</sub>是确定的数据，w是方向确定下，长度可以伸缩的，这意味着我们一定可以做到令分子为1。</p>
<p><img src="/./../pic/image-20231215111935354.png" alt="image-20231215111935354"></p>
<p>​	对于有约束条件的求最值，比较常用的就是拉格朗日乘子法，上式等于：</p>
<p><img src="/./../pic/image-20231215112542616.png" alt="image-20231215112542616"></p>
<p>​	其中λ是拉格朗日乘子，此时我们可以发现S<sub>b</sub>w的方向是恒为μ<sub>0</sub>-μ<sub>1</sub>，此时不妨令：</p>
<p><img src="/./../pic/image-20231215113932906.png" alt="image-20231215113932906"></p>
<p>​	代入拉格朗日乘子法，则可得：</p>
<p><img src="/./../pic/image-20231215114001371.png" alt="image-20231215114001371"></p>
<p>​	实际计算的时候需要注意：</p>
<p><img src="/./../pic/image-20231215114731362.png" alt="image-20231215114731362"></p>
<p><img src="/./../pic/image-20231215130626604.png" alt="image-20231215130626604"></p>
<h3 id="补充：奇异值分解"><a href="#补充：奇异值分解" class="headerlink" title="补充：奇异值分解"></a>补充：奇异值分解</h3><p>​	是矩阵特征值分解的一种延伸，矩阵可以不是方阵。</p>
<p>​	具体逻辑跳过，计算步骤为：</p>
<p>​	1.计算矩阵A<sup>T</sup>A，对结果进行特征值分解，所有特征向量组成的矩阵（n*n)就是对应的矩阵V</p>
<p>​	2.计算矩阵AA<sup>T</sup>，对结果进行特征值分解，所有特征向量组成的矩阵（n*n)就是对应的矩阵U</p>
<p>​	3.Σ是一个对角阵，所以直接根据已经算出来的UV计算每一个奇异值就可以了。</p>
<p><img src="/./../pic/v2-eab35f0f8896ebe2dbf64d3c0b2bb1da_r.jpg" alt="img"></p>
<p>​	举个栗子：</p>
<p><img src="/./../pic/v2-1eded2adc70ac4eab0afe6ec52d31892_1440w.png" alt="img"></p>
<p><img src="/./../pic/v2-8f78e1cf021f78a16bf559243fa16a87_1440w.webp" alt="img"></p>
<p><img src="/./../pic/v2-e36d0129dcd8f95d7f053c85b81387bb_1440w.webp" alt="img"></p>
<p><img src="/./../pic/v2-cacc77aeccd8e54811601e467ce8c786_1440w.png" alt="img"></p>
<p><img src="/./../pic/v2-6f6d014782dd412075f653658bd63bf0_1440w.webp" alt="img"></p>
<p><img src="/./../pic/v2-7517b4e0cdd9a1adcf4cdb42bf27162c_1440w.webp" alt="img"></p>
<h3 id="LDA的扩充"><a href="#LDA的扩充" class="headerlink" title="LDA的扩充"></a>LDA的扩充</h3><p>​	我们前面提到的部分都是二分类任务，但是显然线性判别分类只分两类没道理的。</p>
<p>​	在多分类的任务重，假定存在N个类吧，此时，我们需要在S<sub>b</sub>和S<sub>w</sub>的基础上，补充一个定义“全局散度矩阵”S<sub>t</sub>：</p>
<p><img src="/./../pic/image-20231215150743829.png" alt="image-20231215150743829"></p>
<p>​	在我们前面的定义中S<sub>b</sub>的定义是类间散度矩阵，S<sub>w</sub>是类内散度矩阵，在只有两分类的情况下，直接基于均值和协方差计算即可。</p>
<p>​	但是多分类的情况下，这两者的定义扩充，此时可以记作：</p>
<p><img src="/./../pic/image-20231215150959421.png" alt="image-20231215150959421"></p>
<p>​	其中S<sub>wi</sub>对应第wi类型的协方差矩阵。</p>
<p>​	关于类简单度矩阵基于前面全局散度的计算前提（此处推导不难，就是很麻烦），m<sub>i</sub>表示这个类别的样本数：</p>
<p><img src="/./../pic/image-20231215151008486.png" alt="image-20231215151008486"></p>
<p>​	然后，我们对于多分类LDA，其实使用我们计算出来的三个参数中的两个就可以计算出来了，常用的是：</p>
<p><img src="/./../pic/image-20231215151402988.png" alt="image-20231215151402988"></p>
<p>​	然后计算方式和我们之前二分类那里类似：</p>
<p><img src="/./../pic/image-20231215151436645.png" alt="image-20231215151436645"></p>
<p>​	顺便一提，这也是一个很经典的监督降维技术。</p>
<h2 id="感知器算法"><a href="#感知器算法" class="headerlink" title="感知器算法"></a>感知器算法</h2><p>​	神经网络和支持向量机的基础。是一种二分类的线性分类模型，输入特征向量，输出实例类别。</p>
<p><img src="/./../pic/image-20231216100919068.png" alt="image-20231216100919068"></p>
<p>​	判别规则：</p>
<p><img src="/./../pic/image-20231216101457609.png" alt="image-20231216101457609"></p>
<p>​	我们可以发现，样本分类*预测结果&gt;0则说明分类正确，反之错误，我们基于此得出目标函数：</p>
<p><img src="/./../pic/image-20231216101550245.png" alt="image-20231216101550245"></p>
<p>​	其中M为错误样本的集合，我们希望错误样本尽可能少。这里的话也可以改成增广向量的形式，不过意思都一样。</p>
<p>​	对于这个目标函数，一般使用梯度下降法进行计算。</p>
<h3 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h3><p>​	对于函数J(w)，其极值点只能在边界点，不可导点或者导数为0的点，其中导数为0的点成为临界点。</p>
<p>​	对于多元函数，则是梯度为0。</p>
<p>​	我们可以通过梯度最快下降的方式，找到对应的梯度为0的点。具体方法为：</p>
<p><img src="/./../pic/image-20231216102356788.png" alt="image-20231216102356788"></p>
<p>​	关于收敛性的证明：</p>
<p><img src="/./../pic/image-20231216103516462.png" alt="image-20231216103516462"></p>
<p>​	回到感知器算法，在前面计算的过程中，我们提到了计算目标函数的梯度，其梯度的计算结果为：</p>
<p><img src="/./../pic/image-20231216103609345.png" alt="image-20231216103609345"></p>
<p>​	将这个公式代入到我们前面提到的梯度下降流程中，则计算感知器算法权重w的流程为：</p>
<p><img src="/./../pic/image-20231216103736653.png" alt="image-20231216103736653"></p>
<p>​	再举个栗子：</p>
<p><img src="/./../pic/image-20231216105753349.png" alt="image-20231216105753349"></p>
<p><img src="/./../pic/image-20231216105759997.png" alt="image-20231216105759997"></p>
<p><img src="/./../pic/image-20231216105846414.png" alt="image-20231216105846414"></p>
<p><img src="/./../pic/image-20231216105856852.png" alt="image-20231216105856852"></p>
<p><img src="/./../pic/image-20231216105907878.png" alt="image-20231216105907878"></p>
<h2 id="最小平方误差分类器"><a href="#最小平方误差分类器" class="headerlink" title="最小平方误差分类器"></a>最小平方误差分类器</h2><p>​	在我们前面提到的感知器算法，他可以找到一个判别界面将样本按照类别进行区分，但是显然，这个功能的前提是可分。</p>
<p>​	如果不可分呢？想想我们上面的例子，如果交换一下24的类别，这个梯度下降法一辈子都不会收敛，白费功夫罢了。</p>
<p>​	所以，我们又引入了LMSE算法，对于可分收敛，同时可以指示出来不可分的情况。</p>
<p>​	基于二分类的情况，对问题进行分析：</p>
<p><img src="/./../pic/image-20231216113102005.png" alt="image-20231216113102005"></p>
<p>​	我们的目标就是找到一个w，可以满足我们最后提出的不等式，但是显然我们计算的结果，应该是一个列数组，我们需要每一个元素都&gt;0，所以也可以记作：</p>
<p><img src="/./../pic/image-20231216114051672.png" alt="image-20231216114051672"></p>
<p>​	对于这个准则函数，我们需要求解的部分是w,b，要求是这个准则函数尽可能小且b的元素都要&gt;0.</p>
<p><img src="/./../pic/image-20231216114533549.png" alt="image-20231216114533549"></p>
<p>​	这里b是用的迭代法求解：</p>
<p><img src="/./../pic/image-20231216115438108.png" alt="image-20231216115438108"></p>
<p><img src="/./../pic/image-20231216115447178.png" alt="image-20231216115447178"></p>
<p><img src="/./../pic/image-20231216115458838.png" alt="image-20231216115458838"></p>
<p><img src="/./../pic/image-20231216115514414.png" alt="image-20231216115514414"></p>
<p>​	在迭代和计算的过程中，若模式可分，e<sup>(t)</sup>会逐渐变小（学习率在0-1之间），并逐渐接近0.</p>
<p>​	基于前面迭代计算，评估模式的可分性：</p>
<p><img src="/./../pic/image-20231216115706092.png" alt="image-20231216115706092"></p>
<h2 id="分段线性判别函数"><a href="#分段线性判别函数" class="headerlink" title="分段线性判别函数"></a>分段线性判别函数</h2><p>​	主要在于依稀线性不可分的情况，使用核函数之类的升维方式的话，又过于复杂，就可以使用分段线性判别，用判别平面组合的方式提升判别平面的细节进而实现可分。</p>
<h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><p>​	也是一类常见的机器学习方法，基于树结构进行决策，通过对属性进行判断最后给出结果，如下图：</p>
<p><img src="/./../pic/image-20231216131508639.png" alt="image-20231216131508639"></p>
<h3 id="属性划分"><a href="#属性划分" class="headerlink" title="属性划分"></a>属性划分</h3><p>​	基于我们对于决策树朴素的理解，我们可以意识到，对于决策树来说，每一个节点进行判断的选择是很重要的，我们肯定不希望做出判断之后发现数据根本没有分开。最理想的情况就是判断之后，一半类别1，一半类别2，直接大功告成。</p>
<p>​	所以，选择最优划分属性是很重要的。我们希望决策树的分支节点所包含的样本尽可能属于同一个类别，也就是纯度越来越高。</p>
<p>​	1.信息增益</p>
<p>​	基于我们的目的，我们首先引入信息熵的概念，这是一个度量样本集合纯度的最常用指标。</p>
<p><img src="/./../pic/image-20231216131811411.png" alt="image-20231216131811411"></p>
<p>​	这个Ent(D)越小纯度对应越高。假设现在有一个离散的属性a有V个不同的选择，那么选择这个属性来划分的话，会产生V个分支节点。对于这V个分支节点，以样本数做权重，加权计算信息熵的和。</p>
<p><img src="/./../pic/image-20231216132743616.png" alt="image-20231216132743616"></p>
<p>​	这里的信息增益越大，说明划分提纯效果越好。</p>
<p>​	2.增益率</p>
<p>​	我们前面提到的信息增益是一个有效的判断属性划分的方法，但是我们也同样知道，就是对于一组数据来说，如果我们豁得出去，直接按照主键来进行划分的话，最后的结果就是一个样本对应一个叶子节点，这个信息增益直接无敌了。但是这好吗？这显然是没有任何道理的，我按照编号学号划分，毫无意义，面对新的数据一碰就碎。</p>
<p>​	所以，我们还需要对信息增益进行补充，不能抛开剂量谈毒性，我们进一步引入增益率的概念：</p>
<p><img src="/./../pic/image-20231216134350787.png" alt="image-20231216134350787"></p>
<p>​	IV(a)称之为是属性A的固有值，一般来讲，这个属性可以选择的数目越多，这个数值就会越大。</p>
<p>​	3.基尼指数</p>
<p>​	独立于我们前面提到的信息增益和增益率的概念的另一个评估数据集纯度的指标，优点就是不用log计算，会快一些。</p>
<p><img src="/./../pic/image-20231216134753414.png" alt="image-20231216134753414"></p>
<p>​	换个说法，基尼指数就是从数据集里面随便取出来两个样本，标记不一致的概率，这个概率小就说明纯度高。对于用来划分的属性，基尼指数的定义为：</p>
<p>​	<img src="/./../pic/image-20231216134843406.png" alt="image-20231216134843406"></p>
<p>​	这个地方基尼指数是越小越好。</p>
<h3 id="剪枝处理"><a href="#剪枝处理" class="headerlink" title="剪枝处理"></a>剪枝处理</h3><p>​	为什么要剪枝，什么是剪枝？</p>
<p>​	剪枝是一种应对与过拟合的手段，我们理论上可以搞一个无限多个属性划分的树，这个树巨大无比，我训练集上每一个数据都是对的。但是代价就是，叶子结点多且杂，九成九过拟合了。</p>
<p>​	对于决策树模型，相同效果，树结构越简单越好，剪枝就是干这个的。</p>
<p>​	剪枝又可以分作预剪枝和后剪枝，前者就是划分之前估计一下能不能提升泛化性能，不能就算了。后者就是我树都搭建好了，从下往上看看能不能砍两刀下来提升泛化能力。</p>
<p>​	这两个方法都提到了泛化能力，这个东西概念简单，但是我们怎么判断呢？比较可靠的一个法子就是，留一部分数据出来做验证集进行评估。</p>
<p>​	整体预剪枝的流程如下：</p>
<p>​	1.一开始，训练数据集都在一起，此时我们只有一个根节点，这个节点的判断结果取集合中多数数据的标签。有一个精度</p>
<p>​	2.选择一个属性，划分出来若干个子节点，此时基于属性和节点内容，给出判断结果，如果精度提升，就执行反正打住。</p>
<p>​	3.知道所有节点都不可以在划分，结束此操作。</p>
<p>​	后剪枝操作类似，就是从下往上来，也是以提高验证集精度为第一目的。后剪枝的决策树往往结构更复杂一些，也有更好的泛化性能，但是问题是时间开销，需要先把树搭建出来。然后再进行剪枝操作，预剪枝剪完树也建好了，两者各有优劣。</p>
<h3 id="连续值处理"><a href="#连续值处理" class="headerlink" title="连续值处理"></a>连续值处理</h3><p>​	离散的数据，我们前面这一套下来，也就差不多了。但是连续的往往就麻烦一些。</p>
<p>​	最明显的，我们不能直接根据取值划分节点了。此时，可以选择的最简单的方案就是二分法，随便找个阈值来一刀，按相比阈值大小划分，这个阈值一般来讲就是取已有训练数据的中间值，1,3,5,7,9就分别去拿2,4,6,8作为阈值试一下，找效果最好的。</p>
<p>​	评估效果的话，上面三个都行。</p>
<h3 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a>缺失值处理</h3><p>​	样本的某些属性值缺失，但是这个样本其他数据又不能浪费。所以还是得用。</p>
<p>​	那用也有两个问题，一个是我这个属性没有数据，按这个属性划分的时候我算什么？另一个是，我这个属性没有，那你选属性来划分的时候，算不算我？</p>
<p>​	首先选择属性进行划分，我们可以只使用没有缺失值的样本子集，但是在具体操作上会有一些不同。</p>
<p>​	具体的区别的点在于一些之前的定义参数：</p>
<p><img src="/./../pic/image-20231216144238905.png" alt="image-20231216144238905"></p>
<p>​	基于这些参数补充，推广之前的评估指标：</p>
<p><img src="/./../pic/image-20231216144204947.png" alt="image-20231216144204947"></p>
<p><img src="/./../pic/image-20231216144320375.png" alt="image-20231216144320375"></p>
<p>​	基于以上操作，就可以完成评估。找到了划分的属性之后，就是对于该属性没有数据的样本怎么进行划分的问题了。</p>
<p>​	这里的操作比较魔幻，但是确实是有道理的，我们不知道的数据，直接分到所有子节点上，因为你不是，所以你什么都可以是。并且我们对于这个样本的权值，要根据那些有属性数据的样本数目调整。（成了一个概率问题）</p>
<p>​	</p>
<h1 id="考点"><a href="#考点" class="headerlink" title="考点"></a>考点</h1><p>​	梯度下降法</p>
<p>​	（决策树不考）	</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/01/31/%E5%88%A4%E5%88%AB%E5%BC%8F%E5%88%86%E7%B1%BB%E5%99%A8/" data-id="cls1gaxjh0000wwue4a9bb2so" data-title="判别式分类器" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%9B%BD%E7%A7%91%E5%A4%A7%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A023%E7%A7%8B%E5%AD%A3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" rel="tag">国科大模式识别与机器学习23秋季学习笔记</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2024/01/31/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          神经网络
        
      </div>
    </a>
  
  
    <a href="/2024/01/31/%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">降维与度量学习</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AF%BE%E7%A8%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">课程学习笔记</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%9B%BD%E7%A7%91%E5%A4%A7%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A023%E7%A7%8B%E5%AD%A3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" rel="tag">国科大模式识别与机器学习23秋季学习笔记</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/%E5%9B%BD%E7%A7%91%E5%A4%A7%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A023%E7%A7%8B%E5%AD%A3%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" style="font-size: 10px;">国科大模式识别与机器学习23秋季学习笔记</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/01/">January 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/01/31/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/">线性模型</a>
          </li>
        
          <li>
            <a href="/2024/01/31/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络</a>
          </li>
        
          <li>
            <a href="/2024/01/31/%E5%88%A4%E5%88%AB%E5%BC%8F%E5%88%86%E7%B1%BB%E5%99%A8/">判别式分类器</a>
          </li>
        
          <li>
            <a href="/2024/01/31/%E9%99%8D%E7%BB%B4%E4%B8%8E%E5%BA%A6%E9%87%8F%E5%AD%A6%E4%B9%A0/">降维与度量学习</a>
          </li>
        
          <li>
            <a href="/2024/01/31/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/">集成学习</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>